'''
æ€»ç»“ï¼šTransformer åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„ä¼˜åŠ¿

ä¼˜åŠ¿	è¯´æ˜
å¼ºå¤§åºåˆ—å»ºæ¨¡èƒ½åŠ›	é€‚åˆå»ºæ¨¡æ”»å‡»é“¾æ¡ã€ç”¨æˆ·è¡Œä¸ºã€ç½‘ç»œæµç­‰æ—¶åºæ•°æ®
å¤šæ¨¡æ€å…¼å®¹	èƒ½å¤„ç†æ—¥å¿—ã€æµé‡ã€æ–‡ä»¶ã€æ–‡æœ¬ç­‰å¤šç§æ•°æ®ç±»å‹
æ˜“äºé¢„è®­ç»ƒ & è¿ç§»å­¦ä¹ 	å¯å¾®è°ƒ BERT æˆ– ViT ç­‰å®‰å…¨ç›¸å…³ä»»åŠ¡
æ›´å¼ºçš„ä¸Šä¸‹æ–‡ç†è§£	è¯†åˆ«å¤æ‚è¡Œä¸ºæ¨¡å¼ï¼ˆå¦‚ APTã€ç¤¾ä¼šå·¥ç¨‹æ”»å‡»ï¼‰

æ¨¡å‹æ€»ç»“ï¼šå¸¸ç”¨äºç½‘ç»œå®‰å…¨çš„ Transformer ç±»å‹

åç§°	ç”¨é€”	æ¥æº / æ¨¡å‹ç±»å‹
LogBERT	æ—¥å¿—å¼‚å¸¸æ£€æµ‹	ç»“æ„åŒ–æ—¥å¿— + BERT
AnomalyTransformer	æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹	æ”¹è¿› attention + series decomposition
TransIDS	å…¥ä¾µæ£€æµ‹	ç½‘ç»œæµé‡åˆ†ç±»
Malformer	æ¶æ„è½¯ä»¶æ£€æµ‹	äºŒè¿›åˆ¶åºåˆ—å»ºæ¨¡
EMBERT	å¯æ‰§è¡Œæ–‡ä»¶è¯­ä¹‰è¡¨ç¤º	äºŒè¿›åˆ¶é¢„è®­ç»ƒæ¨¡å‹
CyberBERT	å®‰å…¨è¯­ä¹‰åˆ†æï¼ˆCVE/IOTï¼‰	é¢†åŸŸçŸ¥è¯†å¢å¼ºç‰ˆ BERT

æ€»ç»“ï¼šTransformer åœ¨å®‰é˜²é¢†åŸŸçš„é«˜é¢‘ç”¨é€”

å®‰é˜²æ–¹å‘	æŠ€æœ¯æ ¸å¿ƒ	æ˜¯å¦å·²æœ‰éƒ¨ç½²
é™æ€å›¾åƒè¯†åˆ«	ViT / DETR / Swin Transformer	âœ… å¤§é‡éƒ¨ç½²
è§†é¢‘è¡Œä¸ºè¯†åˆ«	TimeSformer / ViViT	â³ æ­£åœ¨å¿«é€Ÿéƒ¨ç½²
å£°éŸ³åˆ†æ	AST / Whisper / Wav2Vec	âœ… æœ‰ç”¨ä¾‹ï¼ˆå¦‚æªå£°æ£€æµ‹ï¼‰
å¤šæ¨¡æ€èåˆ	CLIP / GPT-4V / BLIP-2	â³ å‘å±•ä¸­ï¼ˆAI é©±åŠ¨æ™ºèƒ½å®‰é˜²ï¼‰

å¼€æºæ¡†æ¶æ¨è

æ¡†æ¶ / é¡¹ç›®	ç”¨é€”
MMSelfSup	æ”¯æŒ ViT è§†è§‰é¢„è®­ç»ƒ
MMDetection	DETR/YOLO å®‰é˜²å›¾åƒæ£€æµ‹
TimeSformer	è§†é¢‘ Transformer
BLIP-2	å›¾æ–‡è”åˆåˆ†æ

å…·ä½“æ¨¡å‹æ¨èï¼ˆæŒ‰ä»»åŠ¡ï¼‰

å®‰é˜²ä»»åŠ¡	æ¨èæ¨¡å‹ï¼ˆç°æˆ or å¾®è°ƒï¼‰
å›¾åƒä¸­æ£€æµ‹æ­¦å™¨	âœ… ViT + Faster R-CNNã€YOLOv8 + ViTã€DETR
è§†é¢‘ä¸­è¯†åˆ«æ”»å‡»è¡Œä¸º	âœ… TimeSformerã€VideoMAEã€ViViT
æªå£°æ£€æµ‹	âœ… ASTï¼ˆAudio Spectrogram Transformerï¼‰
å›¾æ–‡è”åˆç†è§£æŠ¥è­¦ä¿¡æ¯	âœ… CLIPã€BLIP-2ã€GPT-4V

æ€»ç»“ï¼šTransformer çš„æœ€å¤šç”¨é€”åˆ†ç±»

åˆ†ç±»é¢†åŸŸ	ç”¨é€”å…³é”®è¯	æ¨¡å‹ä»£è¡¨
è‡ªç„¶è¯­è¨€å¤„ç†	æ–‡æœ¬ç”Ÿæˆã€ç†è§£ã€é—®ç­”ã€ç¿»è¯‘	GPT, BERT, T5, BART
è®¡ç®—æœºè§†è§‰	å›¾åƒåˆ†ç±»ã€æ£€æµ‹ã€ç”Ÿæˆ	ViT, Swin, DETR, DALLÂ·E
å¤šæ¨¡æ€	å›¾æ–‡åŒ¹é…ã€è§†é¢‘ç†è§£ã€éŸ³é¢‘åˆ†æ	CLIP, GPT-4V, Gemini, BLIP
ç¼–ç¨‹é¢†åŸŸ	è‡ªåŠ¨å†™ä»£ç ã€ä»£ç ç†è§£	Codex, CodeLlama
æ—¶é—´åºåˆ—	é¢„æµ‹ã€å¼‚å¸¸æ£€æµ‹ã€ä¿¡å·å»ºæ¨¡	Informer, Autoformer
æ¨èç³»ç»Ÿ	ç”¨æˆ·åºåˆ—å»ºæ¨¡	BERT4Rec, SASRec

å®é™…ä¸­å¯ç”¨çš„æ—¶é—´åºåˆ— Transformer æ¨¡å‹

æ¨¡å‹	ç‰¹ç‚¹	ç”¨é€”
Informer	ç”¨ç¨€ç–æ³¨æ„åŠ›å»ºæ¨¡é•¿åºåˆ—ï¼Œé«˜æ•ˆé¢„æµ‹	é¢„æµ‹ã€æ£€æµ‹
Autoformer	è‡ªé€‚åº”åˆ†è§£æ—¶é—´åºåˆ—ï¼Œè¯†åˆ«è¶‹åŠ¿/å­£èŠ‚æ€§	å¤æ‚æ—¶é—´åºåˆ—é¢„æµ‹
Anomaly Transformer	æ˜¾æ€§å»ºæ¨¡æ­£å¸¸ä¸å¼‚å¸¸ä¹‹é—´çš„â€œå…³è”å¼ºåº¦â€	å¼ºåŒ–å¼‚å¸¸æ£€æµ‹
TS-T5	ç”¨äºæ—¶é—´åºåˆ—â€œè¯­è¨€åŒ–â€ç†è§£ + å¤šä»»åŠ¡é¢„æµ‹	é¢„æµ‹ + è§£é‡Šæ€§ä»»åŠ¡

Input Sequence [x1, x2, ..., xT]
     â†“
+-------------------------+
| Embedding + Positional |
+-------------------------+
     â†“
Transformer Encoder Stack (Self-Attention)
     â†“
     â†“
Detection Head / Forecasting Head
'''
class TimeEmbedding(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.proj = nn.Linear(input_dim, d_model)

    def forward(self, x):
        # x shape: [batch, seq_len, input_dim]
        return self.proj(x)  # [batch, seq_len, d_model]
def sinusoidal_position_encoding(seq_len, d_model):
    import math
    pos = torch.arange(seq_len).unsqueeze(1)
    i = torch.arange(d_model).unsqueeze(0)
    angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model)
    angle_rads = pos * angle_rates
    pe = torch.zeros(seq_len, d_model)
    pe[:, 0::2] = torch.sin(angle_rads[:, 0::2])
    pe[:, 1::2] = torch.cos(angle_rads[:, 1::2])
    return pe  # shape: [seq_len, d_model]

'''
encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=4)
transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)
'''

class ClassifierHead(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.fc = nn.Linear(d_model, 2)

    def forward(self, x):  # x shape: [batch, seq_len, d_model]
        cls_token = x[:, 0, :]  # å‡è®¾ç”¨ç¬¬ä¸€ä¸ª token è¡¨ç¤ºå…¨åºåˆ—
        return self.fc(cls_token)  # [batch, 2]

import torch
import torch.nn as nn

class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=3):
        super().__init__()
        self.embed = nn.Linear(input_dim, d_model)
        self.pos_embed = nn.Parameter(torch.randn(1, seq_len, d_model))
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.head = nn.Linear(d_model, 1)  # äºŒåˆ†ç±»ï¼šå¼‚å¸¸/æ­£å¸¸

    def forward(self, x):  # x: [batch, seq_len, input_dim]
        x = self.embed(x)  # [batch, seq_len, d_model]
        #x = embed(x) + pos_embed  # [batch, seq_len, d_model]
        x = x.permute(1, 0, 2)  # Transformer expects [seq_len, batch, d_model]
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # Back to [batch, seq_len, d_model]
        logits = self.head(x)  # [batch, seq_len, 1]
        return logits.squeeze(-1)  # æ¯ä¸ªæ—¶é—´ç‚¹ä¸€ä¸ªå¼‚å¸¸æ¦‚ç‡

#ä»£ç ç¤ºä¾‹ï¼šç®€æ˜“ Transformer æ—¥å¿—å¼‚å¸¸æ£€æµ‹
'''
æ¨èæ•°æ®é›†

åç§°	æè¿°	é“¾æ¥
HDFS	Hadoop æ—¥å¿—ï¼Œæœ‰æ ‡ç­¾	https://github.com/logpai/loghub
BGL	BlueGene/L ç³»ç»Ÿæ—¥å¿—	åŒä¸Š
Thunderbird	NASA ç³»ç»Ÿæ—¥å¿—	åŒä¸Š
LANL	ç™»å½•/è®¤è¯äº‹ä»¶ï¼ˆæ”»å‡»/æ­£å¸¸ï¼‰	https://csr.lanl.gov/data/cyber1/

ä¸€å¥è¯æ€»ç»“ï¼š

âœ… å°å‹ Transformerï¼ˆLogBERT ç­‰ï¼‰é€‚åˆç»“æ„åŒ–æ—¥å¿—å»ºæ¨¡å’Œé«˜æ•ˆéƒ¨ç½²ï¼›
âœ… LLM æ›´é€‚åˆå¤šæ ·åŒ–ã€éç»“æ„åŒ–ã€å¤æ‚è¯­ä¹‰çš„æ—¥å¿—åˆ†æå’Œè¯­ä¹‰æ¨ç†ã€‚

åœºæ™¯é€‚é…å»ºè®®

åœºæ™¯	æ¨èæ¨¡å‹	ç†ç”±
âœ… é«˜ååæ—¥å¿—ç³»ç»Ÿï¼ˆå¦‚å¤§æ•°æ®å¹³å°ï¼‰	LogBERT / AnomalyTransformer	é«˜æ•ˆï¼Œç»“æ„åŒ–ï¼Œéƒ¨ç½²æ–¹ä¾¿
âœ… å·¥ä¸šæ§åˆ¶ç³»ç»Ÿæ—¥å¿—ï¼ˆå°‘æ ·æœ¬ï¼‰	Self-Attention + One-Class	è®­ç»ƒæ•°æ®ç¨€ç¼ºï¼Œç»“æ„æ¸…æ™°
âœ… åŸå§‹ã€éç»“æ„æ—¥å¿—	GPT-4 / Claude / LLama2-Chat	èƒ½ç›´æ¥ç†è§£è‡ªç„¶è¯­è¨€æ—¥å¿—
âœ… é«˜çº§å®‰å…¨åˆ†æï¼ˆå¨èƒæº¯æºï¼‰	GPT + Prompt / RAG	å¯å›ç­”â€œæ”»å‡»å‘ç”Ÿåœ¨å“ªâ€â€œå¼‚å¸¸åŸå› æ˜¯ä»€ä¹ˆâ€
âœ… è‡ªåŠ¨ SOC å·¥å•ç”Ÿæˆ	GPT + æ—¥å¿—æ€»ç»“ Prompt	å¯ç”Ÿæˆå¤„ç†å»ºè®®ã€äº‹ä»¶æ‘˜è¦

æ€»ç»“

æ¨è	è¯´æ˜
LogBERT / AnomalyTransformer	ğŸš€ é«˜æ•ˆã€æˆç†Ÿã€ç»“æ„åŒ–ï¼Œé€‚åˆå·¥ä¸šéƒ¨ç½²
GPT-4 / Claude / LLaMA	ğŸ§  æ™ºèƒ½è¯­ä¹‰åˆ†æï¼Œé€‚åˆé«˜çº§å¼‚å¸¸è§£é‡Šã€çµæ´»åˆ†æåœºæ™¯
'''

import torch
import torch.nn as nn

class LogTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Parameter(torch.randn(1, 512, d_model))  # max_len=512

        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.fc = nn.Linear(d_model, 1)  # output: anomaly score (regression or sigmoid)

    def forward(self, x):  # x: [batch, seq_len]
        x = self.embedding(x) + self.pos_encoding[:, :x.size(1), :]
        x = self.encoder(x)  # [batch, seq_len, d_model]
        x_cls = x[:, -1, :]  # use last token's output
        return torch.sigmoid(self.fc(x_cls))
'''
æ€»ç»“

æ¨è	è¯´æ˜
LogBERT / AnomalyTransformer	ğŸš€ é«˜æ•ˆã€æˆç†Ÿã€ç»“æ„åŒ–ï¼Œé€‚åˆå·¥ä¸šéƒ¨ç½²
GPT-4 / Claude / LLaMA	ğŸ§  æ™ºèƒ½è¯­ä¹‰åˆ†æï¼Œé€‚åˆé«˜çº§å¼‚å¸¸è§£é‡Šã€çµæ´»åˆ†æåœºæ™¯

è¯·æ±‚åºåˆ—è½¬åŒ–ä¸º Embedding è¾“å…¥
ç¦»æ•£ç‰¹å¾ï¼ˆå¦‚è¯·æ±‚è·¯å¾„ã€UAã€æ–¹æ³•ï¼‰ âœ ç”¨ embedding
æ•°å€¼ç‰¹å¾ï¼ˆè¯·æ±‚é—´éš”ã€payload å¤§å°ï¼‰âœ æ ‡å‡†åŒ–è¾“å…¥
æ—¶é—´ç‰¹å¾ï¼ˆå¦‚å°æ—¶ã€å‘¨å‡ ï¼‰ âœ sin/cos æˆ– one-hot ç¼–ç 

äº”ã€è‡ªç›‘ç£è®­ç»ƒæŠ€å·§ï¼ˆæ— æ ‡ç­¾ä¹Ÿèƒ½è®­ç»ƒï¼‰

Masked Log Modelingï¼ˆMLMï¼‰ï¼šåƒ BERT ä¸€æ ·éšæœº mask æ—¥å¿— tokenï¼Œé¢„æµ‹å®ƒ
Next Log Predictionï¼šè¾“å…¥å‰ N æ¡æ—¥å¿—ï¼Œé¢„æµ‹ç¬¬ N+1 æ¡çš„æ¨¡æ¿ ID
Sequence Validity Classificationï¼šåˆ¤æ–­ä¸€ä¸ªåºåˆ—æ˜¯å¦æ˜¯â€œæ­£å¸¸æ—¥å¿—é¡ºåºâ€

 æ¨èå¼€æºæ–¹æ¡ˆ

é¡¹ç›®/åº“	åŠŸèƒ½
BotD by Fingerprint	æµè§ˆå™¨æŒ‡çº¹è¯†åˆ«ï¼Œæ£€æµ‹è‡ªåŠ¨åŒ–å·¥å…·
OpenWAF	æ”¯æŒåŸºäºè§„åˆ™+è¡Œä¸ºæ£€æµ‹çš„ Web é˜²ç«å¢™
DeepLog	æ—¥å¿—åºåˆ— Transformer å¼‚å¸¸æ£€æµ‹
ELK Stack	æ—¥å¿—é‡‡é›†ã€å±•ç¤ºã€é…åˆæ¨¡å‹ä½¿ç”¨

æ€»ç»“

ç›®æ ‡	æ¨èæ–¹æ¡ˆ
é«˜é¢‘ Bot æ£€æµ‹	Transformer æ¨¡å‹ + è¯·æ±‚åºåˆ—å»ºæ¨¡
å¤šæ¨¡æ€ Bot æ£€æµ‹	Mouse + JS + Log å¤šæ¨¡æ€ Transformer
å¯è§£é‡Šæ€§å¢å¼º	LLM + Prompt ç”Ÿæˆå¼‚å¸¸è§£é‡Š
åŸå§‹ UA/Headers æ£€æµ‹	å¾®è°ƒ BERT/GPT æ¨¡å‹ï¼Œç†è§£ Header ç‰¹å¾è¯­ä¹‰
'''
class BotRequestTransformer(nn.Module):
    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=2):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        self.cls = nn.Linear(d_model, 1)  # äºŒåˆ†ç±»è¾“å‡ºï¼šæ˜¯å¦ä¸º Bot

    def forward(self, x):  # x shape: [batch, seq_len, input_dim]
        x = self.input_proj(x)
        x = self.encoder(x)
        out = x[:, -1]  # æœ€åä¸€ä¸ªè¯·æ±‚è¡¨ç¤ºæ•´ä½“è¡Œä¸º
        return torch.sigmoid(self.cls(out))

'''
å°†ç¦»æ•£ç‰¹å¾ï¼ˆcategorical featuresï¼‰è½¬ä¸º embedding æ˜¯ç°ä»£æœºå™¨å­¦ä¹ ç‰¹åˆ«æ˜¯åœ¨ Transformerã€DNN ä¸­çš„æ ¸å¿ƒæ­¥éª¤ä¹‹ä¸€
'''
import torch
import torch.nn as nn

embedding = nn.Embedding(num_embeddings=4, embedding_dim=8)  # 4 ç±» UA â†’ 8 ç»´ embedding
input = torch.tensor([1])  # UA æ˜¯ "Safari"
embedded = embedding(input)  # è¾“å‡ºå½¢çŠ¶: [1, 8]

'''
å¤šä¸ªç¦»æ•£ç‰¹å¾è”åˆå»ºæ¨¡
'''
class CategoricalEmbedder(nn.Module):
    def __init__(self):
        super().__init__()
        self.ua_embed = nn.Embedding(100, 16)
        self.geo_embed = nn.Embedding(50, 8)
        self.method_embed = nn.Embedding(5, 4)

    def forward(self, ua_id, geo_id, method_id):
        ua_vec = self.ua_embed(ua_id)
        geo_vec = self.geo_embed(geo_id)
        method_vec = self.method_embed(method_id)
        return torch.cat([ua_vec, geo_vec, method_vec], dim=-1)  # [batch_size, total_dim]

# æ¯ä¸ªæ—¶é—´æ­¥çš„ embedding ç»´åº¦ = è¿ç»­ç‰¹å¾ + ç¦»æ•£ embedding æ€»ç»´åº¦
step_input = torch.cat([
    numerical_features,       # [batch, seq_len, num_numerical_features]
    categorical_embed_output  # [batch, seq_len, embedding_dim]
], dim=-1)

# æŠ•å½±åˆ° Transformer çš„ d_model ç»´åº¦
x = self.linear_input(step_input)

'''
nn.Embedding æ˜¯ PyTorch ä¸­ç”¨äº**å°†ç¦»æ•£ç±»åˆ«æ˜ å°„ä¸ºè¿ç»­å‘é‡ï¼ˆembedding å‘é‡ï¼‰**çš„ä¸€ä¸ªç¥ç»ç½‘ç»œå±‚ã€‚
æ˜¯çš„ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œnn.Embedding é‡Œçš„å‚æ•°ä¼šéšç€æ¨¡å‹çš„è®­ç»ƒä¸€èµ·è¢«ä¼˜åŒ–ã€‚è¿™å°±æ˜¯å®ƒçš„å¼ºå¤§ä¹‹å¤„ â€”â€” å®ƒä¸ä»…æ˜¯ä¸€ä¸ªæŸ¥è¡¨æ“ä½œï¼Œè€Œä¸”æ˜¯ä¸€ä¸ª**â€œå¯å­¦ä¹ â€çš„æŸ¥è¡¨å±‚**
'''

import torch
import torch.nn as nn

embedding = nn.Embedding(num_embeddings=4, embedding_dim=8)

input = torch.tensor([2])  # è¡¨ç¤º "Safari"
output = embedding(input)

print(output.shape)  # è¾“å‡º: torch.Size([1, 8])

